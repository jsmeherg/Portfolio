---
title: "HW 1"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(multcomp)
library(gridExtra) 
library(car)
library(lmtest)
library(MASS)
library(magrittr) 
```

#1

```{r}
#1
salarydata <- read.csv("Salary.csv", header = TRUE)
ggplot(data=salarydata, mapping=aes(x=GPA,y=Salary)) + geom_point()
ggplot(data=salarydata, aes(x=Gen,y=Salary)) + geom_boxplot()
ggplot(data=salarydata, mapping=aes(x=MajorCategory,y=Salary)) + geom_boxplot() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```
There appears to be some correlation between GPA and Salary after graduation. There appears that men on average make more than females, there also appears that some majors make more than others.

#2
The linear regression model is y = X B + e, where y is a n x 1 vector of the salaries, X is a n x (P + 1) matrix of the explanatory variables, B is the (P + 1) x 1 vector of coefficients, and e is the n x 1 vector of residuals. Where e is distributed normally with a mean of 0 and variance of sigma-squared * I, where I is the n x n identity matrix.

By making statistical inference on the B we will see how GPA, gender and major affect salary 5 years after graduation.

#3

```{r}
#3
X <- model.matrix(Salary~.,data=salarydata)
y <- salarydata[['Salary']]
beta.hat <- solve(t(X)%*%X)%*%t(X)%*%y
lm.betahat <- coef(lm(Salary~.,data=salarydata))
rownames(beta.hat) <- colnames(X)
cbind(beta.hat,lm.betahat)
```

The left shows the betahat coefficients using first principles, the right is the betahats using lm(). The effect of Biology and Life science majors on the salary differs by $769.1305 on average compared to Agriculture which is the baseline, or they make on average $769.1305 more than agriculture students, while holding all other variables constant. As GPA goes up by 1, the average salary for students, holding all else constant, goes up by $5488.7368.

#4

```{r}
ggplot(data=salarydata, mapping=aes(x=MajorCategory, y=Salary, color=Gen)) + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
salary.lm.int <- lm(Salary~MajorCategory*Gen+GPA,data=salarydata)
salary.lm.no.int <- lm(Salary~MajorCategory+Gen+GPA,data=salarydata)
anova(salary.lm.no.int,salary.lm.int)
```
The boxplot shows a clear possible interaction between gender and major affecting salary, as there are some majors that have differing salaries depending on gender, while other majors have similar average salaries across gender.

To determine this interaction I ran an F test between MajorCategory and Gender interaction and their affect on salary to determine if there was an interaction. The null hypothethis is that there is no interaction between major and gender, and the alternative hypothesis is that there is at least one interaction between major and gender. The interaction f test gives an f-statistic of 4.3595 with a p-value of essentially zero, rejecting the null hypothesis and determining that there is an interaction between at least one major and gender. Thus there is some form of gender discrimination.

#5

```{r}
avPlots(salary.lm.int)
qplot(stdres(salary.lm.int), geom="histogram")
ks.test(stdres(salary.lm.int), "pnorm")
qplot(fitted(salary.lm.int), stdres(salary.lm.int), geom="point") +
  geom_hline(yintercept=0, color="red")
bptest(salary.lm.int)
```


Linearity - the relationship between y and all explanatory variables is linear (only applies to quantitative explanatory variables).
Independence - the salary of one person does not impact the salary of another.
Normality - the residuals e are normall distributed.
Equal variance - the residuals have equal variation across the whole fitted line.

Linearity - the av plots appear to be linear across the variables.
Independence - There is no information showing that there is dependence across the different salaries, thus we can assume that there is independence.
Normality - the histogram  of standardized residuals appears to be normal and the ks test passes.
Equal variance - fitted values vs. standardized residuals appears to have same spread and the BP test passes.

#6

```{r}
confint(salary.lm.int, level=0.97)
```

As GPA goes up by 1 I am 97% confident that average salary increases between $4646.39854 and $6129.7553.
For men in agriculture(the baseline), I am 97% confident that average salary is between $9395.5671 and $24387.6262 higher than for women.
For women majoring in social science, I am 97% confident that the average salary increases between $7854.4475 and $15903.2493, compared to women in the agriculture major.


#7
For the null hypothesis, B(GenM) + B(CompMath:GenM) = 0, and alternative, B(GenM) + B(CompMath:GenM) > 0, we perform a t test. The p-value is close to zero and thus we can reject the null hypothesis and determine that men on average make more than women in the Computers and Mathematics major category.

```{r}
a.glht <- matrix(0, nrow=length(coef(salary.lm.int)), ncol=1)
a.glht[names(coef(salary.lm.int))%in%c("GenM","MajorCategoryComputers & Mathematics:GenM")] <- 1
summary(glht(salary.lm.int, linfct=t(a.glht), alternative="greater"))
```


```{r}
confint(glht(salary.lm.int, linfct=t(a.glht)))
```

For a 95% confidence interval for how much men make more than women, it is: (4339.6592, 11468.8276) dollars.

#8
```{r}
predict.lm(salary.lm.int, newdata=data.frame(Gen="M",MajorCategory="Computers & Mathematics", GPA=2.9),
           interval="prediction")
```
My predicted salary is $87,774.11 5 years after graduation.
My 95% prediction interval for salary 5 years after graduation is: (77108.96,98439.27)

#9
```{r}
rpmse <- rep(NA,nrow(salarydata))
wid <- rep(NA,nrow(salarydata))
for(cv in 1:nrow(salarydata)){
  train.lm <- lm(Salary~MajorCategory*Gen+GPA,data=salarydata[-cv,])
  preds <- predict.lm(train.lm, newdata=salarydata[cv,], interval="prediction")
  rpmse[cv] <- (salarydata[cv,'Salary']-preds[,'fit'])^2 %>% mean() %>% sqrt()
  wid[cv] <- preds[,'upr'] - preds[,'lwr'] %>% mean()
}
mean(rpmse)
```

```{r}
mean(wid)
```
With a leave one out cross validation the RPMSE is about 4358 with a width of 21013.43, although the rpmse is only off by the range of salary by a small amount, the width suggests that there is much uncertainty that suggest the predictions are not very accurate.
